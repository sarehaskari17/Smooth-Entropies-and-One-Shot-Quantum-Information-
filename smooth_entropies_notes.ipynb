{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pPsyZxiELtMM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def normalize(p):\n",
        "    \"\"\"Normalize a non-negative vector to a probability distribution.\"\"\"\n",
        "    p = np.array(p, dtype=float)\n",
        "    if np.any(p < 0):\n",
        "        raise ValueError(\"Probabilities must be non-negative.\")\n",
        "    Z = p.sum()\n",
        "    if Z == 0:\n",
        "        raise ValueError(\"Sum of probabilities is zero.\")\n",
        "    return p / Z\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def H_shannon(p):\n",
        "    \"\"\"Shannon entropy in bits.\"\"\"\n",
        "    p = normalize(p)\n",
        "    # avoid log(0) by masking zero entries\n",
        "    mask = p > 0\n",
        "    return -np.sum(p[mask] * np.log2(p[mask]))\n",
        "\n",
        "def H_min(p):\n",
        "    \"\"\"Min-entropy in bits: -log2(max_x p_x).\"\"\"\n",
        "    p = normalize(p)\n",
        "    return -np.log2(np.max(p))\n",
        "\n",
        "def H_max(p):\n",
        "    \"\"\"Max-entropy (Renner-style) in bits: 2 * log2(sum_x sqrt(p_x)).\"\"\"\n",
        "    p = normalize(p)\n",
        "    return 2 * np.log2(np.sum(np.sqrt(p)))\n"
      ],
      "metadata": {
        "id": "JeKbH-coMBMh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: fair coin\n",
        "p_fair = [0.5, 0.5]\n",
        "\n",
        "print(\"Fair coin:\")\n",
        "print(\"H_shannon =\", H_shannon(p_fair))\n",
        "print(\"H_min     =\", H_min(p_fair))\n",
        "print(\"H_max     =\", H_max(p_fair))\n",
        "\n",
        "# Example 2: biased coin\n",
        "p_biased = [0.9, 0.1]\n",
        "\n",
        "print(\"\\nBiased coin:\")\n",
        "print(\"H_shannon =\", H_shannon(p_biased))\n",
        "print(\"H_min     =\", H_min(p_biased))\n",
        "print(\"H_max     =\", H_max(p_biased))\n",
        "\n",
        "# Example 3: four-outcome distribution\n",
        "p_four = [0.5, 0.2, 0.2, 0.1]\n",
        "\n",
        "print(\"\\nFour-outcome example:\")\n",
        "print(\"H_shannon =\", H_shannon(p_four))\n",
        "print(\"H_min     =\", H_min(p_four))\n",
        "print(\"H_max     =\", H_max(p_four))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgAbDTMNMUP2",
        "outputId": "9acb620e-8943-4f8f-8572-505ea15c37a3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fair coin:\n",
            "H_shannon = 1.0\n",
            "H_min     = 1.0\n",
            "H_max     = 1.0000000000000002\n",
            "\n",
            "Biased coin:\n",
            "H_shannon = 0.4689955935892812\n",
            "H_min     = 0.15200309344504995\n",
            "H_max     = 0.6780719051126377\n",
            "\n",
            "Four-outcome example:\n",
            "H_shannon = 1.7609640474436812\n",
            "H_min     = 0.9999999999999997\n",
            "H_max     = 1.8788469835018302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EJs2dWxPMb6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def H_min_smooth_toy(p, eps):\n",
        "    \"\"\"\n",
        "    Toy version of smooth min-entropy.\n",
        "\n",
        "    Idea: we allow ourselves to discard events whose total probability is up to eps,\n",
        "    starting from the least likely outcomes. This is NOT the formal definition, but\n",
        "    gives some intuition for how smoothing can increase H_min.\n",
        "    \"\"\"\n",
        "    p = normalize(p)\n",
        "    # sort probabilities ascending\n",
        "    p_sorted = np.sort(p)\n",
        "    remaining = 1.0\n",
        "    i = 0\n",
        "    # remove small probabilities until we've removed at most eps\n",
        "    to_remove = eps\n",
        "    while i < len(p_sorted) and to_remove > 0:\n",
        "        if p_sorted[i] <= to_remove:\n",
        "            to_remove -= p_sorted[i]\n",
        "            p_sorted[i] = 0.0\n",
        "        else:\n",
        "            # partially reduce this probability\n",
        "            p_sorted[i] -= to_remove\n",
        "            to_remove = 0.0\n",
        "        i += 1\n",
        "\n",
        "    # renormalize the trimmed distribution\n",
        "    p_trimmed = normalize(p_sorted)\n",
        "    return H_min(p_trimmed)\n"
      ],
      "metadata": {
        "id": "PYGTMbYNMWw7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_example = [0.6, 0.2, 0.15, 0.05]\n",
        "\n",
        "print(\"Original distribution:\", normalize(p_example))\n",
        "print(\"Original H_min =\", H_min(p_example))\n",
        "\n",
        "for eps in [0.0, 0.01, 0.05, 0.1, 0.2]:\n",
        "    print(f\"eps = {eps:0.2f}, H_min_smooth_toy = {H_min_smooth_toy(p_example, eps):0.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXN67teZMhD7",
        "outputId": "33015311-fe9d-424f-fe7c-6f73bc41ded1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original distribution: [0.6  0.2  0.15 0.05]\n",
            "Original H_min = 0.7369655941662062\n",
            "eps = 0.00, H_min_smooth_toy = 0.7370\n",
            "eps = 0.01, H_min_smooth_toy = 0.7225\n",
            "eps = 0.05, H_min_smooth_toy = 0.6630\n",
            "eps = 0.10, H_min_smooth_toy = 0.5850\n",
            "eps = 0.20, H_min_smooth_toy = 0.4150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def P_guess(p):\n",
        "    \"\"\"Optimal guessing probability: max_x p_x.\"\"\"\n",
        "    p = normalize(p)\n",
        "    return np.max(p)\n",
        "\n",
        "for p in [p_fair, p_biased, p_four]:\n",
        "    print(\"p =\", normalize(p))\n",
        "    print(\"P_guess =\", P_guess(p))\n",
        "    print(\"H_min   =\", H_min(p))\n",
        "    print(\"2^{-H_min} =\", 2 ** (-H_min(p)))\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4W8XQD2PMjRo",
        "outputId": "8666546c-3afd-4f4c-eb39-bfe7ff4a7fc6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p = [0.5 0.5]\n",
            "P_guess = 0.5\n",
            "H_min   = 1.0\n",
            "2^{-H_min} = 0.5\n",
            "----------------------------------------\n",
            "p = [0.9 0.1]\n",
            "P_guess = 0.9\n",
            "H_min   = 0.15200309344504995\n",
            "2^{-H_min} = 0.9\n",
            "----------------------------------------\n",
            "p = [0.5 0.2 0.2 0.1]\n",
            "P_guess = 0.5000000000000001\n",
            "H_min   = 0.9999999999999997\n",
            "2^{-H_min} = 0.5000000000000001\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jto3v642Mlkg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}